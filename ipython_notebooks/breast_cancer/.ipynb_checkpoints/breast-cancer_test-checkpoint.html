

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Logistic Regression Applied to Classification of Breast Tumors &mdash; macaw 0.1dev0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../../index.html" class="icon icon-home"> macaw
          

          
            
            <img src="../../../_static/macaw_small.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1dev0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../install.html#development-version">Development version</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/index.html">API documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/optimizers.html">Optimizers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/optimizers.html#inheritance-diagram">Inheritance Diagram</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/objective_functions.html">Objective Functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/objective_functions.html#inheritance-diagram">Inheritance Diagram</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../index.html">IPython notebooks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../robust_linear_l1norm.html">Robust fitting of linear models using Least Absolute Deviations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../l1_l2norm_linear_regression.html">L1 and L2 Linear Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../logistic_regression.html">Logistic Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../breast-cancer.html">Logistic Regression Applied to Classification of Breast Tumors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../breast-cancer.html#1.-Data-Visualization">1. Data Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../breast-cancer.html#2.-Model-fitting">2. Model fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../breast-cancer.html#3.-Comparison-against-scikit-learn">3. Comparison against scikit-learn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../breast-cancer.html#4.-Logistic-Regression-with-L1-Regularization">4. Logistic Regression with L1 Regularization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../abalone/abalone.html">Predicting the ages of abalones using Linear Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../abalone/abalone.html#1.-Data-Visualization">1. Data Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../abalone/abalone.html#2.-Model-fitting">2. Model fitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../abalone/abalone.html#3.-Comparison-against-scikit-learn">3. Comparison against scikit-learn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../abalone/abalone.html#4.-Linear-Regression-with-L1-Regularization">4. Linear Regression with L1 Regularization</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">macaw</a>
        
      </nav>


      
      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
      <li>Logistic Regression Applied to Classification of Breast Tumors</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/ipython_notebooks/breast_cancer/.ipynb_checkpoints/breast-cancer_test-checkpoint.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="Logistic-Regression-Applied-to-Classification-of-Breast-Tumors">
<h1>Logistic Regression Applied to Classification of Breast Tumors<a class="headerlink" href="#Logistic-Regression-Applied-to-Classification-of-Breast-Tumors" title="Permalink to this headline">¶</a></h1>
<p>In this notebook, we use logistic regression to classify breast tumors
in two classes, benign or malignant. The dataset used in this short
tutorial is available here:
<a class="reference external" href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/">https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/</a>.
<em>Note: there were a few missing data (label as ‘?’) which were replaced
with zeros</em>.</p>
<p>The whole documentation of the dataset can be seen in the
<code class="docutils literal notranslate"><span class="pre">breast-cancer-wisconsin.names</span></code> file available in the link above.
Nonetheless, I will briefly mention the characteristics of this dataset.</p>
<p>This dataset has nine interger-valued features that biologically
characterizes a given tumor, e.g., size of the cell, clump thickness,
etc. Every sample in the dataset has a label (or <code class="docutils literal notranslate"><span class="pre">class</span></code>) which
indicates whether the tumor is benign or malignant. Benign samples have
<code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">==</span> <span class="pre">2</span></code> whereas malignant samples have <code class="docutils literal notranslate"><span class="pre">class</span> <span class="pre">==</span> <span class="pre">4</span></code>.</p>
<div class="section" id="1.-Data-Visualization">
<h2>1. Data Visualization<a class="headerlink" href="#1.-Data-Visualization" title="Permalink to this headline">¶</a></h2>
<p>Let’s load and visualize the dataset using Pandas</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Sample code number&#39;</span><span class="p">,</span> <span class="s1">&#39;Clump Thickness&#39;</span><span class="p">,</span> <span class="s1">&#39;Uniformity of Cell Size&#39;</span><span class="p">,</span>
          <span class="s1">&#39;Uniformity of Cell Shape&#39;</span><span class="p">,</span> <span class="s1">&#39;Marginal Adhesion&#39;</span><span class="p">,</span> <span class="s1">&#39;Single Epithelial Cell Size&#39;</span><span class="p">,</span>
          <span class="s1">&#39;Bare Nuclei&#39;</span><span class="p">,</span> <span class="s1">&#39;Bland Chromatin&#39;</span><span class="p">,</span> <span class="s1">&#39;Normal Nucleoli&#39;</span><span class="p">,</span> <span class="s1">&#39;Mitoses&#39;</span><span class="p">,</span> <span class="s1">&#39;Class&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">breast_cancer_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;breast-cancer-wisconsin.data&#39;</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">breast_cancer_df</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sample code number</th>
      <th>Clump Thickness</th>
      <th>Uniformity of Cell Size</th>
      <th>Uniformity of Cell Shape</th>
      <th>Marginal Adhesion</th>
      <th>Single Epithelial Cell Size</th>
      <th>Bare Nuclei</th>
      <th>Bland Chromatin</th>
      <th>Normal Nucleoli</th>
      <th>Mitoses</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1000025</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1002945</td>
      <td>5</td>
      <td>4</td>
      <td>4</td>
      <td>5</td>
      <td>7</td>
      <td>10</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1015425</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1016277</td>
      <td>6</td>
      <td>8</td>
      <td>8</td>
      <td>1</td>
      <td>3</td>
      <td>4</td>
      <td>3</td>
      <td>7</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1017023</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1017122</td>
      <td>8</td>
      <td>10</td>
      <td>10</td>
      <td>8</td>
      <td>7</td>
      <td>10</td>
      <td>9</td>
      <td>7</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1018099</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>10</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1018561</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1033078</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1033078</td>
      <td>4</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>10</th>
      <td>1035283</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>11</th>
      <td>1036172</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>12</th>
      <td>1041801</td>
      <td>5</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1043999</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>14</th>
      <td>1044572</td>
      <td>8</td>
      <td>7</td>
      <td>5</td>
      <td>10</td>
      <td>7</td>
      <td>9</td>
      <td>5</td>
      <td>5</td>
      <td>4</td>
      <td>4</td>
    </tr>
    <tr>
      <th>15</th>
      <td>1047630</td>
      <td>7</td>
      <td>4</td>
      <td>6</td>
      <td>4</td>
      <td>6</td>
      <td>1</td>
      <td>4</td>
      <td>3</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>16</th>
      <td>1048672</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>17</th>
      <td>1049815</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>18</th>
      <td>1050670</td>
      <td>10</td>
      <td>7</td>
      <td>7</td>
      <td>6</td>
      <td>4</td>
      <td>10</td>
      <td>4</td>
      <td>1</td>
      <td>2</td>
      <td>4</td>
    </tr>
    <tr>
      <th>19</th>
      <td>1050718</td>
      <td>6</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>20</th>
      <td>1054590</td>
      <td>7</td>
      <td>3</td>
      <td>2</td>
      <td>10</td>
      <td>5</td>
      <td>10</td>
      <td>5</td>
      <td>4</td>
      <td>4</td>
      <td>4</td>
    </tr>
    <tr>
      <th>21</th>
      <td>1054593</td>
      <td>10</td>
      <td>5</td>
      <td>5</td>
      <td>3</td>
      <td>6</td>
      <td>7</td>
      <td>7</td>
      <td>10</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>22</th>
      <td>1056784</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>23</th>
      <td>1057013</td>
      <td>8</td>
      <td>4</td>
      <td>5</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>7</td>
      <td>3</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>24</th>
      <td>1059552</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>25</th>
      <td>1065726</td>
      <td>5</td>
      <td>2</td>
      <td>3</td>
      <td>4</td>
      <td>2</td>
      <td>7</td>
      <td>3</td>
      <td>6</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>26</th>
      <td>1066373</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>27</th>
      <td>1066979</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>28</th>
      <td>1067444</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>29</th>
      <td>1070935</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>669</th>
      <td>1350423</td>
      <td>5</td>
      <td>10</td>
      <td>10</td>
      <td>8</td>
      <td>5</td>
      <td>5</td>
      <td>7</td>
      <td>10</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>670</th>
      <td>1352848</td>
      <td>3</td>
      <td>10</td>
      <td>7</td>
      <td>8</td>
      <td>5</td>
      <td>8</td>
      <td>7</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>671</th>
      <td>1353092</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>672</th>
      <td>1354840</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>673</th>
      <td>1354840</td>
      <td>5</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>674</th>
      <td>1355260</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>675</th>
      <td>1365075</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>676</th>
      <td>1365328</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>677</th>
      <td>1368267</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>678</th>
      <td>1368273</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>679</th>
      <td>1368882</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>680</th>
      <td>1369821</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>5</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>7</td>
      <td>4</td>
    </tr>
    <tr>
      <th>681</th>
      <td>1371026</td>
      <td>5</td>
      <td>10</td>
      <td>10</td>
      <td>10</td>
      <td>4</td>
      <td>10</td>
      <td>5</td>
      <td>6</td>
      <td>3</td>
      <td>4</td>
    </tr>
    <tr>
      <th>682</th>
      <td>1371920</td>
      <td>5</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>683</th>
      <td>466906</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>684</th>
      <td>466906</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>685</th>
      <td>534555</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>686</th>
      <td>536708</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>687</th>
      <td>566346</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>3</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>688</th>
      <td>603148</td>
      <td>4</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>689</th>
      <td>654546</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>690</th>
      <td>654546</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>691</th>
      <td>695091</td>
      <td>5</td>
      <td>10</td>
      <td>10</td>
      <td>5</td>
      <td>4</td>
      <td>5</td>
      <td>4</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>692</th>
      <td>714039</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>693</th>
      <td>763235</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>694</th>
      <td>776715</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>695</th>
      <td>841769</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>696</th>
      <td>888820</td>
      <td>5</td>
      <td>10</td>
      <td>10</td>
      <td>3</td>
      <td>7</td>
      <td>3</td>
      <td>8</td>
      <td>10</td>
      <td>2</td>
      <td>4</td>
    </tr>
    <tr>
      <th>697</th>
      <td>897471</td>
      <td>4</td>
      <td>8</td>
      <td>6</td>
      <td>4</td>
      <td>3</td>
      <td>4</td>
      <td>10</td>
      <td>6</td>
      <td>1</td>
      <td>4</td>
    </tr>
    <tr>
      <th>698</th>
      <td>897471</td>
      <td>4</td>
      <td>8</td>
      <td>8</td>
      <td>5</td>
      <td>4</td>
      <td>5</td>
      <td>10</td>
      <td>4</td>
      <td>1</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
<p>699 rows × 11 columns</p>
</div></div>
</div>
<p>Let’s take a look at the distribution of the dataset:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">benign_samples</span> <span class="o">=</span> <span class="n">breast_cancer_df</span><span class="p">[</span><span class="n">breast_cancer_df</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">malignant_samples</span> <span class="o">=</span> <span class="n">breast_cancer_df</span><span class="p">[</span><span class="n">breast_cancer_df</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">4</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Percentage of benign examples: </span><span class="si">{}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">benign_samples</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">breast_cancer_df</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Percentage of benign examples: 66.0%
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Percentage of malignant examples: </span><span class="si">{}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">malignant_samples</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">breast_cancer_df</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Percentage of malignant examples: 34.0%
</pre></div></div>
</div>
</div>
<div class="section" id="2.-Model-fitting">
<h2>2. Model fitting<a class="headerlink" href="#2.-Model-fitting" title="Permalink to this headline">¶</a></h2>
<p>Let’s use Scikit-learn to split the dataset in training set and testing
set:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">breast_cancer_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;Clump Thickness&#39;</span><span class="p">:</span><span class="s1">&#39;Mitoses&#39;</span><span class="p">],</span>
                                                    <span class="n">breast_cancer_df</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Note that I scaled the <code class="docutils literal notranslate"><span class="pre">'Class'</span></code> label such that <code class="docutils literal notranslate"><span class="pre">0</span></code> represents
benign sample and <code class="docutils literal notranslate"><span class="pre">1</span></code> represents malignant samples. This has to be
done solely because of the assumptions of the logistic regression
algorithm implemented in <code class="docutils literal notranslate"><span class="pre">macaw</span></code>.</p>
<p>Now, let’s import the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> objective function from
<code class="docutils literal notranslate"><span class="pre">macaw</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">macaw.objective_functions</span> <span class="k">import</span> <span class="n">LogisticRegression</span>
</pre></div>
</div>
</div>
<p>See
<a class="reference external" href="https://mirca.github.io/macaw/api/objective_functions.html#macaw.objective_functions.LogisticRegression">https://mirca.github.io/macaw/api/objective_functions.html#macaw.objective_functions.LogisticRegression</a>
for documentation.</p>
<p>Let’s instantiate an object from <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> passing the
labels <code class="docutils literal notranslate"><span class="pre">y_train</span></code> and the features <code class="docutils literal notranslate"><span class="pre">X_train</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">X</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let’s use the method <code class="docutils literal notranslate"><span class="pre">fit</span></code> to get the maximum likelihood weights.</p>
<p><em>Note that we need to pass an initial estimate for the linear weights
and bias of the ``LogisiticRegression``</em>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">res</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x0</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(159)compute()
-&gt; while i &lt; n:
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(160)compute()
-&gt; x_tmp = x0
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(161)compute()
-&gt; fun_before = self.fun.evaluate(x0)
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(162)compute()
-&gt; self.optimizer.compute(x0=x0, fun_args=x0, **kwargs)
(Pdb) s
--Call--
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(35)compute()
-&gt; def compute(self, x0, fun_args=(), n=1000, xtol=1e-6, ftol=1e-9):
(Pdb) s
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(36)compute()
-&gt; import pdb; pdb.set_trace()
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(37)compute()
-&gt; fun = _wrap_function(self.fun, fun_args)
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(39)compute()
-&gt;
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(40)compute()
-&gt; i = 0
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(41)compute()
-&gt; while i &lt; n:
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(42)compute()
-&gt; x_tmp = x0
(Pdb) x0
array([ 0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1,  0.1])
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(43)compute()
-&gt; fun_before = fun(x0)
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(44)compute()
-&gt; grad = fun_prime(x0)
(Pdb) fun_before
591.10384676871672
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(45)compute()
-&gt; x0 = x0 - self.gamma * grad
(Pdb) grad
array([ 807.88497756,  358.48159564,  398.71692314,  370.63986078,
        574.42853285,  355.72793531,  563.8514626 ,  356.34819978,
        287.19143071,  267.25515037])
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(46)compute()
-&gt; fun_after = fun(x0)
(Pdb) x0
array([-0.70788498, -0.2584816 , -0.29871692, -0.27063986, -0.47442853,
       -0.25572794, -0.46385146, -0.2563482 , -0.18719143, -0.16725515])
(Pdb) self.gamma
0.001
(Pdb) l
 41             while i &lt; n:
 42                 x_tmp = x0
 43                 fun_before = fun(x0)
 44                 grad = fun_prime(x0)
 45                 x0 = x0 - self.gamma * grad
 46  -&gt;                  fun_after = fun(x0)
 47                 grad_diff = fun_prime(x0) - grad
 48                 self.gamma = np.dot(x0 - x_tmp, grad_diff) / np.dot(grad_diff, grad_diff)
 49
 50                 if abs((fun_after - fun_before) / (1.+fun_before)) &lt; ftol:
 51                     msg = (&#34;Success: loss function has not changed by {} since&#34;
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(47)compute()
-&gt; grad_diff = fun_prime(x0) - grad
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(49)compute()
-&gt;
(Pdb) grad_diff
array([-1971.07865283, -1418.55952011, -1433.59569973, -1302.58779149,
       -1433.01934288, -1612.79225417, -1523.73235151, -1284.39641212,
        -708.23669905,  -427.3984274 ])
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(55)compute()
-&gt;
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(60)compute()
-&gt; break
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(40)compute()
-&gt; i = 0
(Pdb) l
 35         def compute(self, x0, fun_args=(), n=1000, xtol=1e-6, ftol=1e-9):
 36             import pdb; pdb.set_trace()
 37             fun = _wrap_function(self.fun, fun_args)
 38             fun_prime = _wrap_function(self.gradient, fun_args)
 39
 40  -&gt;              i = 0
 41             while i &lt; n:
 42                 x_tmp = x0
 43                 fun_before = fun(x0)
 44                 grad = fun_prime(x0)
 45                 x0 = x0 - self.gamma * grad
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(41)compute()
-&gt; while i &lt; n:
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(42)compute()
-&gt; x_tmp = x0
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(43)compute()
-&gt; fun_before = fun(x0)
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(44)compute()
-&gt; grad = fun_prime(x0)
(Pdb) x0
array([-0.70788498, -0.2584816 , -0.29871692, -0.27063986, -0.47442853,
       -0.25572794, -0.46385146, -0.2563482 , -0.18719143, -0.16725515])
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(45)compute()
-&gt; x0 = x0 - self.gamma * grad
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(46)compute()
-&gt; fun_after = fun(x0)
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(47)compute()
-&gt; grad_diff = fun_prime(x0) - grad
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(49)compute()
-&gt;
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(55)compute()
-&gt;
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(60)compute()
-&gt; break
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(40)compute()
-&gt; i = 0
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(41)compute()
-&gt; while i &lt; n:
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(42)compute()
-&gt; x_tmp = x0
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(43)compute()
-&gt; fun_before = fun(x0)
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(44)compute()
-&gt; grad = fun_prime(x0)
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(45)compute()
-&gt; x0 = x0 - self.gamma * grad
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(46)compute()
-&gt; fun_after = fun(x0)
(Pdb) x0
array([ 0.71939338,  1.07576665,  0.98599941,  0.86943837,  0.53934785,
        1.2996173 ,  0.68182089,  0.88920076,  0.30627513, -0.03393391])
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(47)compute()
-&gt; grad_diff = fun_prime(x0) - grad
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(49)compute()
-&gt;
(Pdb) grad_diff
array([ 1828.15079822,  1239.57820138,  1254.7800047 ,  1131.9242768 ,
        1296.15528778,  1368.88511679,  1365.02863359,  1117.44163942,
         642.29483865,   393.29560901])
(Pdb) l
 44                 grad = fun_prime(x0)
 45                 x0 = x0 - self.gamma * grad
 46                 fun_after = fun(x0)
 47                 grad_diff = fun_prime(x0) - grad
 48                 self.gamma = np.dot(x0 - x_tmp, grad_diff) / np.dot(grad_diff, grad_diff)
 49  -&gt;
 50                 if abs((fun_after - fun_before) / (1.+fun_before)) &lt; ftol:
 51                     msg = (&#34;Success: loss function has not changed by {} since&#34;
 52                            &#34; the previous iteration&#34;.format(ftol))
 53                     self.save_state(x0, fun_after, i+1, msg)
 54                     break
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(55)compute()
-&gt;
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(60)compute()
-&gt; break
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(40)compute()
-&gt; i = 0
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(41)compute()
-&gt; while i &lt; n:
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(42)compute()
-&gt; x_tmp = x0
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(43)compute()
-&gt; fun_before = fun(x0)
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(44)compute()
-&gt; grad = fun_prime(x0)
(Pdb) x0
array([ 0.71939338,  1.07576665,  0.98599941,  0.86943837,  0.53934785,
        1.2996173 ,  0.68182089,  0.88920076,  0.30627513, -0.03393391])
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(45)compute()
-&gt; x0 = x0 - self.gamma * grad
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(46)compute()
-&gt; fun_after = fun(x0)
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(47)compute()
-&gt; grad_diff = fun_prime(x0) - grad
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(49)compute()
-&gt;
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(55)compute()
-&gt;
(Pdb) x0
array([ 0.06210622,  0.78402225,  0.66108546,  0.56618636,  0.06822655,
        1.00719205,  0.21882018,  0.59948739,  0.06868666, -0.25527609])
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(60)compute()
-&gt; break
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(40)compute()
-&gt; i = 0
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(41)compute()
-&gt; while i &lt; n:
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(42)compute()
-&gt; x_tmp = x0
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(43)compute()
-&gt; fun_before = fun(x0)
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(44)compute()
-&gt; grad = fun_prime(x0)
(Pdb) n
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(45)compute()
-&gt; x0 = x0 - self.gamma * grad
(Pdb) n
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
/Users/jvmirca/dev/macaw/macaw/objective_functions.py:329: RuntimeWarning: overflow encountered in exp
  return np.nansum((1 - self.y) * l + np.log1p(np.exp(-l)))
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&gt; /Users/jvmirca/dev/macaw/macaw/optimizers.py(46)compute()
-&gt; fun_after = fun(x0)
(Pdb) x0
array([-73.34890277, -31.81941544, -35.65999447, -33.3184346 ,
       -52.50789919, -31.69439309, -51.46901742, -31.77305917,
       -26.42614478, -24.93278399])
(Pdb) self.gamma
0.076272692857834806
(Pdb) grad
array([ 962.4808859 ,  427.4588516 ,  476.20030931,  444.25625597,
        689.31781175,  428.7456482 ,  677.67159732,  424.43167203,
        347.36981803,  323.5431578 ])
(Pdb) q
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">BdbQuit</span>                                   Traceback (most recent call last)
<span class="ansi-green-fg">&lt;ipython-input-33-8aaec6244579&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-fg">()</span>
<span class="ansi-green-fg">----&gt; 1</span><span class="ansi-red-fg"> </span>res <span class="ansi-blue-fg">=</span> logreg<span class="ansi-blue-fg">.</span>fit<span class="ansi-blue-fg">(</span>x0<span class="ansi-blue-fg">=</span>np<span class="ansi-blue-fg">.</span>zeros<span class="ansi-blue-fg">(</span>X_train<span class="ansi-blue-fg">.</span>shape<span class="ansi-blue-fg">[</span><span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">]</span> <span class="ansi-blue-fg">+</span> <span class="ansi-cyan-fg">1</span><span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">+</span> <span class="ansi-cyan-fg">1e-1</span><span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/dev/macaw/macaw/objective_functions.py</span> in <span class="ansi-cyan-fg">fit</span><span class="ansi-blue-fg">(self, x0, n, xtol, ftol, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    366</span>     <span class="ansi-green-fg">def</span> fit<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> x0<span class="ansi-blue-fg">,</span> n<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1000</span><span class="ansi-blue-fg">,</span> xtol<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1e-6</span><span class="ansi-blue-fg">,</span> ftol<span class="ansi-blue-fg">=</span><span class="ansi-cyan-fg">1e-9</span><span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">    367</span>         self<span class="ansi-blue-fg">.</span>mm <span class="ansi-blue-fg">=</span> MajorizationMinimization<span class="ansi-blue-fg">(</span>self<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 368</span><span class="ansi-red-fg">         </span>self<span class="ansi-blue-fg">.</span>mm<span class="ansi-blue-fg">.</span>compute<span class="ansi-blue-fg">(</span>x0<span class="ansi-blue-fg">=</span>x0<span class="ansi-blue-fg">,</span> n<span class="ansi-blue-fg">=</span>n<span class="ansi-blue-fg">,</span> xtol<span class="ansi-blue-fg">=</span>xtol<span class="ansi-blue-fg">,</span> ftol<span class="ansi-blue-fg">=</span>ftol<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    369</span>         <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>mm
<span class="ansi-green-intense-fg ansi-bold">    370</span>

<span class="ansi-green-fg">~/dev/macaw/macaw/optimizers.py</span> in <span class="ansi-cyan-fg">compute</span><span class="ansi-blue-fg">(self, x0, n, xtol, ftol, **kwargs)</span>
<span class="ansi-green-intense-fg ansi-bold">    160</span>             x_tmp <span class="ansi-blue-fg">=</span> x0
<span class="ansi-green-intense-fg ansi-bold">    161</span>             fun_before <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>fun<span class="ansi-blue-fg">.</span>evaluate<span class="ansi-blue-fg">(</span>x0<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">--&gt; 162</span><span class="ansi-red-fg">             </span>self<span class="ansi-blue-fg">.</span>optimizer<span class="ansi-blue-fg">.</span>compute<span class="ansi-blue-fg">(</span>x0<span class="ansi-blue-fg">=</span>x0<span class="ansi-blue-fg">,</span> fun_args<span class="ansi-blue-fg">=</span>x0<span class="ansi-blue-fg">,</span> <span class="ansi-blue-fg">**</span>kwargs<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">    163</span>             x0 <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>optimizer<span class="ansi-blue-fg">.</span>x
<span class="ansi-green-intense-fg ansi-bold">    164</span>             fun_after <span class="ansi-blue-fg">=</span> self<span class="ansi-blue-fg">.</span>fun<span class="ansi-blue-fg">.</span>evaluate<span class="ansi-blue-fg">(</span>x0<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/dev/macaw/macaw/optimizers.py</span> in <span class="ansi-cyan-fg">compute</span><span class="ansi-blue-fg">(self, x0, fun_args, n, xtol, ftol)</span>
<span class="ansi-green-intense-fg ansi-bold">     44</span>             grad <span class="ansi-blue-fg">=</span> fun_prime<span class="ansi-blue-fg">(</span>x0<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     45</span>             x0 <span class="ansi-blue-fg">=</span> x0 <span class="ansi-blue-fg">-</span> self<span class="ansi-blue-fg">.</span>gamma <span class="ansi-blue-fg">*</span> grad
<span class="ansi-green-fg">---&gt; 46</span><span class="ansi-red-fg">             </span>fun_after <span class="ansi-blue-fg">=</span> fun<span class="ansi-blue-fg">(</span>x0<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     47</span>             grad_diff <span class="ansi-blue-fg">=</span> fun_prime<span class="ansi-blue-fg">(</span>x0<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-</span> grad
<span class="ansi-green-intense-fg ansi-bold">     48</span>             self<span class="ansi-blue-fg">.</span>gamma <span class="ansi-blue-fg">=</span> np<span class="ansi-blue-fg">.</span>dot<span class="ansi-blue-fg">(</span>x0 <span class="ansi-blue-fg">-</span> x_tmp<span class="ansi-blue-fg">,</span> grad_diff<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">/</span> np<span class="ansi-blue-fg">.</span>dot<span class="ansi-blue-fg">(</span>grad_diff<span class="ansi-blue-fg">,</span> grad_diff<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/dev/macaw/macaw/optimizers.py</span> in <span class="ansi-cyan-fg">compute</span><span class="ansi-blue-fg">(self, x0, fun_args, n, xtol, ftol)</span>
<span class="ansi-green-intense-fg ansi-bold">     44</span>             grad <span class="ansi-blue-fg">=</span> fun_prime<span class="ansi-blue-fg">(</span>x0<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     45</span>             x0 <span class="ansi-blue-fg">=</span> x0 <span class="ansi-blue-fg">-</span> self<span class="ansi-blue-fg">.</span>gamma <span class="ansi-blue-fg">*</span> grad
<span class="ansi-green-fg">---&gt; 46</span><span class="ansi-red-fg">             </span>fun_after <span class="ansi-blue-fg">=</span> fun<span class="ansi-blue-fg">(</span>x0<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     47</span>             grad_diff <span class="ansi-blue-fg">=</span> fun_prime<span class="ansi-blue-fg">(</span>x0<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">-</span> grad
<span class="ansi-green-intense-fg ansi-bold">     48</span>             self<span class="ansi-blue-fg">.</span>gamma <span class="ansi-blue-fg">=</span> np<span class="ansi-blue-fg">.</span>dot<span class="ansi-blue-fg">(</span>x0 <span class="ansi-blue-fg">-</span> x_tmp<span class="ansi-blue-fg">,</span> grad_diff<span class="ansi-blue-fg">)</span> <span class="ansi-blue-fg">/</span> np<span class="ansi-blue-fg">.</span>dot<span class="ansi-blue-fg">(</span>grad_diff<span class="ansi-blue-fg">,</span> grad_diff<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/lib/python3.6/bdb.py</span> in <span class="ansi-cyan-fg">trace_dispatch</span><span class="ansi-blue-fg">(self, frame, event, arg)</span>
<span class="ansi-green-intense-fg ansi-bold">     46</span>             <span class="ansi-green-fg">return</span> <span class="ansi-red-fg"># None</span>
<span class="ansi-green-intense-fg ansi-bold">     47</span>         <span class="ansi-green-fg">if</span> event <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;line&#39;</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-fg">---&gt; 48</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>dispatch_line<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span>
<span class="ansi-green-intense-fg ansi-bold">     49</span>         <span class="ansi-green-fg">if</span> event <span class="ansi-blue-fg">==</span> <span class="ansi-blue-fg">&#39;call&#39;</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     50</span>             <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>dispatch_call<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">,</span> arg<span class="ansi-blue-fg">)</span>

<span class="ansi-green-fg">~/anaconda3/lib/python3.6/bdb.py</span> in <span class="ansi-cyan-fg">dispatch_line</span><span class="ansi-blue-fg">(self, frame)</span>
<span class="ansi-green-intense-fg ansi-bold">     65</span>         <span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>stop_here<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span> <span class="ansi-green-fg">or</span> self<span class="ansi-blue-fg">.</span>break_here<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span><span class="ansi-blue-fg">:</span>
<span class="ansi-green-intense-fg ansi-bold">     66</span>             self<span class="ansi-blue-fg">.</span>user_line<span class="ansi-blue-fg">(</span>frame<span class="ansi-blue-fg">)</span>
<span class="ansi-green-fg">---&gt; 67</span><span class="ansi-red-fg">             </span><span class="ansi-green-fg">if</span> self<span class="ansi-blue-fg">.</span>quitting<span class="ansi-blue-fg">:</span> <span class="ansi-green-fg">raise</span> BdbQuit
<span class="ansi-green-intense-fg ansi-bold">     68</span>         <span class="ansi-green-fg">return</span> self<span class="ansi-blue-fg">.</span>trace_dispatch
<span class="ansi-green-intense-fg ansi-bold">     69</span>

<span class="ansi-red-fg">BdbQuit</span>:
</pre></div></div>
</div>
<p>The maximum likelihood weights can accessed using the <code class="docutils literal notranslate"><span class="pre">.x</span></code> attribute:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [32]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">res</span><span class="o">.</span><span class="n">x</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[32]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan])
</pre></div>
</div>
</div>
<p>Additionally, we can check the status of the <code class="docutils literal notranslate"><span class="pre">fit</span></code> and the number of
iterations that it took to converge.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">res</span><span class="o">.</span><span class="n">status</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[21]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>&#39;Success: parameters have not changed by 1e-06 since the previous iteration.&#39;
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of iterations needed: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">niters</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Number of iterations needed: 3
</pre></div></div>
</div>
<p>Now, let’s compute the accuracy of our model using the test set. For
that we can use the <code class="docutils literal notranslate"><span class="pre">predict</span></code> method passing the testing samples. This
method outputs the class of each samples:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">logreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
/Users/jvmirca/dev/macaw/macaw/models.py:72: RuntimeWarning: overflow encountered in exp
  return 1 / (1 + np.exp(-self.linear(*theta)))
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[23]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
        0.,  0.])
</pre></div>
</div>
</div>
<p>Now we can compute the percentage of samples correctly classified:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span> <span class="o">==</span> <span class="n">logreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X_test</span><span class="p">)))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">decimals</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The accuracy of the model is </span><span class="si">{}</span><span class="s1">%&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The accuracy of the model is 96.19048%
</pre></div></div>
</div>
</div>
<div class="section" id="3.-Comparison-against-scikit-learn">
<h2>3. Comparison against scikit-learn<a class="headerlink" href="#3.-Comparison-against-scikit-learn" title="Permalink to this headline">¶</a></h2>
<p>Let’s compare <code class="docutils literal notranslate"><span class="pre">macaw</span></code> against <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="k">import</span> <span class="n">LogisticRegression</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">logit</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">logit</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[22]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class=&#39;ovr&#39;, n_jobs=1,
          penalty=&#39;l2&#39;, random_state=None, solver=&#39;liblinear&#39;, tol=0.0001,
          verbose=0, warm_start=False)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>In [23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">logit</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre>
<span></span>Out[23]:
</pre></div>
</div>
<div class="output_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>0.96190476190476193
</pre></div>
</div>
</div>
<p><strong>Looks like</strong> <code class="docutils literal notranslate"><span class="pre">macaw</span></code> <strong>has a good agreement with</strong> <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>
<strong>:)!</strong></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, mirca.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../',
            VERSION:'0.1dev0',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script> 

</body>
</html>